{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _____________________________ 1.2 Classification _____________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### classification is the process of classifying objects into groups based on the values of the features.\n",
    "#### it can be binary classification or multi-class classification\n",
    "#### Confusion Matirx is a table that is used to evaluate the performance of a classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------- <b> 1.2.1 Logistic Regression  </b> ------------------\n",
    "\n",
    "\n",
    "##### Here we try to squash the best fit line from end points of that line in order divide it into 2 classes , that's why it is known as logistic regression \n",
    "\n",
    "##### <h1> h<sub>θ</sub>(x) =g( θ<sub>0</sub> + θ<sub>1</sub>x  )    </h1>\n",
    "##### here g is sigmoid/logistic function and it is used to squash the best fit line from end points of that line in order divide it into 2 classes\n",
    "\n",
    "##### we can say that h<sub>θ</sub>(x) =g(Z)  , where Z =  θ<sub>0</sub> + θ<sub>1</sub>x\n",
    "##### then by applying the function we get ,\n",
    "<h1> h<sub>θ</sub>(x) = 1 / (1 + e<sup>-Z </sup>)  </h1>\n",
    "\n",
    "\n",
    "<img src=\"6.jpg\" height=\"60%\" width=\"40%\">\n",
    "\n",
    "<h3> h<sub>θ</sub>(x) = 1 / (1 + e<sup>-(θ<sub>1</sub>x)  </sup>)  </h3> Considering θ<sub>0</sub> as 0\n",
    "\n",
    "##### but it non-convex function so it leads to local minima problem , so we use some modification to make it convex\n",
    "<img src=\"https://miro.medium.com/max/926/1*ZyjEj3A_QyR4WY7y5cwIWQ.png\" height=\"60%\" width=\"40%\">\n",
    "<img src=\"https://miro.medium.com/max/996/1*TqZ9myxIdLuKNmt8orCeew.png\" height=\"60%\" width=\"40%\">\n",
    "<img src=\"7.jpg\" height=\"60%\" width=\"40%\">\n",
    "\n",
    "### Before using classification , check data is balanced or not , if not then we need to balance it\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  import logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "log_model =  LogisticRegression( solver='lbfgs' , max_iter=1000 , penalty='l2' , C=1)\n",
    "\n",
    "log_model = LogisticRegression()\n",
    "\n",
    "#  here \n",
    "\n",
    "#  solver is the algorithm used to solve the optimization problem\n",
    "#  max_iter is the maximum number of iterations for the solver to run\n",
    "# C is the inverse of regularization strength; must be a positive float.\n",
    "# penalty is the type of regularization used. l1 is the lasso penalty,l2 is ridge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------- <b> 1.2.2 Naive bais  </b> ------------------\n",
    "\n",
    "##### naive bais is a simple model that assumes that the features are independent of each other.\n",
    "##### it is based on bayes theorem and it is used to classify the data. it is given by :\n",
    "<img src=\"https://www.analyticsvidhya.com/wp-content/uploads/2015/09/Bayes_rule-300x172-300x172.png\">\n",
    "\n",
    "#### here P(A) := y = no  and P(B) := P(xi)    i.e  x= 0  to x=i   , here only P(xi) will be constant as it will be ant way canceled in further calculation\n",
    "<img src=\"8.jpg\">\n",
    "\n",
    "##### same as above we calculate P(A) := P(y=yes/x1 to x4)  \n",
    "\n",
    "##### after that  P(yes) = P(y=yes/x1 to x4)  /  P(y=yes/x1 to x4) + P(y=no/x1 to x4)   \n",
    "#####  same with P(no) = P(y=no/x1 to x4)  /  P(y=yes/x1 to x4) + P(y=no/x1 to x4)  but it will be  P(no) = 1 - P(yes) , and which on ein greater , it will be answer\n",
    "##### Naïve: It is called Naïve because it assumes that the occurrence of a certain feature is independent of the occurrence of other features. \n",
    "#### Advantages of Naïve Bayes Classifier\n",
    "#####  fast and easy ML algorithms to predict a class of datasets.\n",
    "#####  Binary as well as Multi-class Classifications.\n",
    "##### used in text classification problems.\n",
    "#### Disadvantages of Naïve Bayes Classifier:\n",
    "##### Naive Bayes assumes that all features are independent or unrelated, so it cannot learn the relationship between features.\n",
    "### Types of Naive Bayes Classifier \n",
    "##### 1. Gaussian Naive Bayes \n",
    "#####   The Gaussian model assumes that features follow a normal distribution. This means if predictors take continuous values instead of discrete, then the model assumes that these values are sampled from the Gaussian distribution.\n",
    "##### 2. Multinomial Naive Bayes\n",
    "#####   The multinomial model is typically used for document classification where the features represent the word frequencies in the document.\n",
    "##### 3. Bernoulli Naive Bayes\n",
    "#####    The Bernoulli classifier works similar to the Multinomial classifier, but the predictor variables are the independent Booleans variables. Such as if a particular word is present or not in a document. This model is also famous for document classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Naive Bayes Modesl: \n",
      " ['BernoulliNB', 'GaussianNB', 'MultinomialNB', 'ComplementNB', 'CategoricalNB']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB  \n",
    "classifier = GaussianNB()  \n",
    "\n",
    "from sklearn import naive_bayes\n",
    "print(\"Available Naive Bayes Modesl: \\n\",naive_bayes.__all__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------- <b> 1.2.3  k Nearest Neighbour  KNN </b> ------------------\n",
    "\n",
    "\n",
    "##### using Euclidean distance to calculate the distance between two points\n",
    "#####  also we can use manhattan distance to calculate the distance between two points\n",
    "##### It is also called a lazy learner algorithm because it does not learn from the training set immediately instead it stores the dataset and at the time of classification, it performs an action on the dataset.\n",
    "##### K-NN is a non-parametric algorithm, which means it does not make any assumption on underlying data.\n",
    "##### K-NN algorithm stores all the available data and classifies a new data point based on the similarity. This means when new data appears then it can be easily classified into a well suite category by using K- NN algorithm.\n",
    "### KNN Steps\n",
    "#### Step-1: Select the number K of the neighbors\n",
    "#### Step-2: Calculate the Euclidean distance of K number of neighbors\n",
    "#### Step-3: Take the K nearest neighbors as per the calculated Euclidean distance.\n",
    "#### Step-4: Among these k neighbors, count the number of the data points in each category.\n",
    "#### Step-6: Our model is ready. \n",
    "#### Step-5: Assign the new data points to that category for which the number of the neighbor is maximum.\n",
    "#### Advantages of KNN \n",
    "##### simple , robust to the noisy training data , effective if training data is large\n",
    "####  Disadvantages of KNN\n",
    "##### Computationally expensive , requires a lot of memory , prediction stage might be slow if training dataset is large \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Nearest Neighbors Models: \n",
      " ['BallTree', 'DistanceMetric', 'KDTree', 'KNeighborsClassifier', 'KNeighborsRegressor', 'KNeighborsTransformer', 'NearestCentroid', 'NearestNeighbors', 'RadiusNeighborsClassifier', 'RadiusNeighborsRegressor', 'RadiusNeighborsTransformer', 'kneighbors_graph', 'radius_neighbors_graph', 'KernelDensity', 'LocalOutlierFactor', 'NeighborhoodComponentsAnalysis', 'VALID_METRICS', 'VALID_METRICS_SPARSE']\n"
     ]
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "print(\"Available Nearest Neighbors Models: \\n\",neighbors.__all__)\n",
    "\n",
    "#  import KNN\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the right K value\n",
    "\n",
    "##### KNN requires a lot of memory because it stores all the training data. So, if the training data is large, then it will take a lot of time and memory to store all the data. So, it is important to choose the right value of K.\n",
    "##### A small value of k means that noise will have a higher influence on the result. A large value make it computationally expensive and kinda defeats the basic philosophy behind KNN (that points that are near might have similar densities or classes ) .A simple approach to select k is set k = n^(1/2).\n",
    "\n",
    "### Using Error rate calculation to find the best K value\n",
    "\n",
    "```python\n",
    "error = []\n",
    "\n",
    "for i in range(1, 40):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train, y_train)\n",
    "    pred_i = knn.predict(X_test)\n",
    "    error.append(np.mean(pred_i != y_test))\n",
    "```\n",
    "\n",
    "```python\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='blue', markersize=10)\n",
    "plt.title('Error Rate K Value')\n",
    "plt.xlabel('K Value')\n",
    "plt.ylabel('Mean Error')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---------- <b> 1.2.4  Decision Tree </b> ------------------\n",
    "\n",
    "##### Decision Tree is a tree-based model for classification and regression.\n",
    "#####  here we have nodes which contains the feature and the value of that feature \n",
    "##### A pure node is node having only one class label. like if we have only two classes then pure node will be having only one class labels.\n",
    "#### Entropy \n",
    "##### Entropy is a measure of how often a randomly chosen element from a set will be labeled as a certain class.\n",
    "##### Entropy is a measure of disorder or uncertainty and the goal of machine learning models and Data Scientists in general is to reduce uncertainty.\n",
    "##### it is given by :\n",
    "<img src=\"https://miro.medium.com/max/782/1*nNY_7_aWRwp8E2DyGduEPg.png\">\n",
    "\n",
    "##### for binary classification , we have 2 classes i.e. yes and no , so entropy is given by :\n",
    "##### H (Entropy) = [ -(P(yes) * log2(P(yes)) ]  - [ P(no) * log2(P(no))) ]\n",
    "##### entropy is zero for pure node , higher the entropy value , more the disorder/impurity in the data\n",
    "##### it generally in between 0 and 1 , but can be greater than 1 and then also it is considerd as 1 \n",
    "#### Gini Impurity\n",
    "#####\n",
    "#####\n",
    "#### Information Gain \n",
    "#####  it helps to decide which feature is the best feature to split the data or which feature will acts as root node and parent node of other nodes.\n",
    "##### Information Gain, or IG for short, measures the reduction in entropy or surprise by splitting a dataset according to a given value of a random variable.\n",
    "###  IG(S ,x) = H(x) -  Σ<sub>v</sub> [ ( |S<sub>v</sub>| / |S| ) *H(x<sub>v</sub>) ]      ---  x is root node , S is feature value of root node\n",
    "##### V belongs to catagory i.e. child node of parent node , Sv is count of childs split count from parents value of feature\n",
    "##### e.g  suppoes we have 2 classes i.e. yes and no , parent node S and 2 childs c1 and c2 \n",
    "##### IG is calculated as given by with feature x we have 9yes and 5 no , c1 has 6yes , 2no and c2 has 3yes , 3no\n",
    "##### H(x) = -(9/14) * log2(9/14) + (5/14) * log2(5/14) = 0.94\n",
    "##### H(c1) = -(6/8) * log2(6/8) + (2/8) * log2(2/8) = 0.81\n",
    "##### H(c2) = -(3/5) * log2(3/5) + (3/5) * log2(3/5) = 1\n",
    "#####  IG(S ,x) = 0.94 - [ 8/14 * 0.81 +  6/14 * 1 ] = 0.041    ---- sv = 8 and 6 as 8 values belongs to c1 and 6 values belongs to c2 out of 14 values\n",
    "#### Gini Impurity \n",
    "#####\n",
    "##### in case of pure nodes gini impurity is 0.5 , entropy is 1\n",
    "##### entropy is costly to calculate as it has log function , so we use gini impurity\n",
    "##### Gini index is a measure of impurity or purity used while creating a decision tree in the CART(Classification and Regression Tree) algorithm.\n",
    "### Gini Index= 1- ∑<sub>j</sub>P<sub>j</sub><sup>2</sup>\n",
    "#####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Decision Tree Models: \n",
      " ['BaseDecisionTree', 'DecisionTreeClassifier', 'DecisionTreeRegressor', 'ExtraTreeClassifier', 'ExtraTreeRegressor', 'export_graphviz', 'plot_tree', 'export_text']\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "print(\"Available Decision Tree Models: \\n\",tree.__all__)\n",
    "\n",
    "#  import Decision Tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_model = DecisionTreeClassifier()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "b82bd812b6a37d04ba59de08f4dfa67a262de1c3e27303c0cb0a5b38b381c61f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
