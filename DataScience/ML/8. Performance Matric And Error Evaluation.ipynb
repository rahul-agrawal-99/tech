{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We use confusion matrix to evaluate the performance of a classification model.\n",
    "##### True Positive: Interpretation: You predicted positive and it’s true.\n",
    "##### True Negative: You predicted negative and it’s true.\n",
    "##### False Positive: (Type 1 Error)   You predicted positive and it’s false.\n",
    "##### False Negative: (Type 2 Error)  You predicted negative and it’s false.\n",
    "\n",
    "\n",
    "<img src=\"https://static.packt-cdn.com/products/9781838555078/graphics/C13314_06_05.jpg\" height=\"40%\" width=\"40%\">\n",
    "\n",
    "### We get Accuracy as : True Positive + True Negetive  /   Total samples\n",
    "## Recall     (For type 2 error)                \n",
    "###   True Positive / (True Positive + False Negative)\n",
    "#### ​Recall: Out of total actual positives, how many predicted are positive .Recall/sensetivity should be high as possible.\n",
    "#### used where higher False Negetive value will be disaster  \n",
    "##### it is considerd priorly in a case like canser detection as having canser but getting false result leads to heavy cost.\n",
    "\n",
    "## Precision      (For type 1 error)                     \n",
    "###  True Positive / (True Positive + False Positive)   \n",
    "#### from all the classes we have predicted as positive, how many are actually positive. Precision should be high as possible.\n",
    "#### used where higher False Positive value will be disaster  \n",
    "##### it should be given priority in a case like spam email \n",
    "\n",
    "### F-Measure-Score is calculated as : (1 + β² ) * (Precision * Recall) /  β²(Precision + Recall)\n",
    "#### It is difficult to compare two models with low precision and high recall or vice versa. So to make them comparable, we use F-Score. F-score helps to measure Recall and Precision at the same time. It uses Harmonic Mean in place of Arithmetic Mean by punishing the extreme values more\n",
    "### in case of F1-score , β = 1 ,i.e. F1-score = (2 * Precision * Recall) / (Precision + Recall)\n",
    "#### if False Positive is more important then we keep  β  value as minimum i.e. β = 0.5 \n",
    "#### if False Negetive is more important then we keep  β  value between 2 to 10 based on impact of false negetive value\n",
    "\n",
    "### We can do this using classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "report = classification_report(y_test, y_pred )   # contains precision, recall, f1-score, support\n",
    "\n",
    "print(report)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)   # contains true positive, false positive, false negative, true negative\n",
    "\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt=\"d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Receiver Operator Characteristic (ROC) curve is an evaluation metric for binary classification problems.\n",
    "\n",
    "##### The ROC (Receiver Operating Characteristics) curve is a plot of the performance of the model (a plot of the true positive rate and the false positive rate) at all classification thresholds. The AUC is the measurement of the entire two-dimensional area under the curve and as such is a measure of the performance of the model at all possible classification thresholds.\n",
    "\n",
    "##### ROC curves plot the accuracy of the model and therefore are best suited to diagnose the performance of models where the data is not imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ROC Curve\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Mean Absolute Error (MAE)\n",
    "\n",
    "#### The mean absolute error is the average of the absolute differences between the predictions and the actual values. It gives an idea of how wrong the predictions were.It is most Robust to outliers.\n",
    "\n",
    "## MAE = 1/n * Σ |y - ŷ|\n",
    "\n",
    "### 2. Mean Squared Error (MSE)\n",
    "\n",
    "#### The mean squared error is the average of the squared differences between the predictions and the actual values. The graph of MSE is differentiable, so you can easily use it as a loss function.\n",
    "#### If you have outliers in the dataset then it penalizes the outliers most and the calculated MSE is bigger. So, in short, It is not Robust to outliers which were an advantage in MAE.\n",
    "\n",
    "## MSE = 1/n * Σ (y - ŷ)²\n",
    "\n",
    "### 3. Root Mean Squared Error (RMSE)\n",
    "\n",
    "#### The root mean squared error is the square root of the average of the squared differences between the predictions and the actual values. It is the most popular metric for regression problems.It is not that robust to outliers as compared to MAE.\n",
    "\n",
    "## RMSE = √(1/n * Σ (y - ŷ)²)\n",
    "\n",
    "### 4. R2 Score\n",
    "\n",
    "#### The R2 score is a statistical measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination, or the coefficient of multiple determination for multiple regression. It is a measure of how well observed outcomes are replicated by the model, based on the proportion of total variation of outcomes explained by the model. It is the most popular metric for regression problems.\n",
    "#### So, with help of R squared we have a baseline model to compare a model which none of the other metrics provides. The same we have in classification problems which we call a threshold which is fixed at 0.5. So basically R2 squared calculates how must regression line is better than a mean line. Hence, R2 squared is also known as Coefficient of Determination or sometimes also known as Goodness of fit\n",
    "\n",
    "## R2 = 1 - (SSres/SStot)\n",
    "\n",
    "### Where SSres is the sum of squared residuals (or sum of squared prediction errors) and SStot is the total sum of squares.\n",
    "#### SSR = Σ (y<sub>i</sub> - ŷ<sub>i</sub>)²    and   SST = Σ (y<sub>i</sub> - y<sub>m</sub>)²    , where y<sub>m</sub> is the mean of y and ŷ is the predicted value of y\n",
    "\n",
    "### 5. Adjusted R2 Score\n",
    "\n",
    "##### The disadvantage of the R2 score is while adding new features in data the R2 score starts increasing or remains constant but it never decreases because It assumes that while adding more data variance of data increases.\n",
    "\n",
    "##### But the problem is when we add an irrelevant feature in the dataset then at that time R2 sometimes starts increasing which is incorrect.\n",
    " \n",
    "##### Hence, To control this situation Adjusted R Squared came into existence.\n",
    "\n",
    "##  R2<sub>adj</sub> = 1 - [(1 - R<sup>2</sup>) * (n - 1) /  (n - p - 1)]\n",
    "#### Where n is the total number of observations and p is the number of predictors(independent variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. MAE: Mean Absolute Error\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# 2. MSE: Mean Squared Error\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "# 3. RMSE: Root Mean Squared Error\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "rmse = sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# 4. R2: R-Squared\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# 5. Accuracy\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "b82bd812b6a37d04ba59de08f4dfa67a262de1c3e27303c0cb0a5b38b381c61f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
