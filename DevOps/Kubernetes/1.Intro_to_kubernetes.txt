************************************************************************************************************************
										About Kubernetes
************************************************************************************************************************


* imperative means use cli every time to execute , declarative means make file and then execute it every time 

Cloud Native : architectural approach conatains workload that is portable ,
				 modular and isolate between differnent cloud providers and cloud deployment models

k8s is based on cloud native

The Linux Foundation :
	formed in 2000's by merger between Open source development labs and Free student groups to support linux's growth and commerercial adoption 

CNCF (Cloud native Computing Foundation):
			formed in 2015 as a linux Foundation project to help advance container technology ,it operates independently from parent orgaization 


K8s , Prometheus , ETCD, ConaitnerD ,HELM  are some of the projects by CNCF

simply container management/orchestration tool ,also known as k8s ,
	Container orchestration automates the deployment,management, scaling, and networking of containers.
orchestration means clustering of any no of containers running on any network , it can works on also hybrid cloud also
Kubernetes was originally developed and designed by engineers at Google. it was developed in Golang , 
	it supports menifest scripts in JSON and Yaml

container orchestration to automate and manage tasks such as:

	Provisioning and deployment
	Configuration and scheduling 
	Resource allocation
	Container availability 
	Scaling or removing containers based on balancing workloads across your infrastructure (can scale vertically[amount of resources] as well horizontally[no of containers])
	Load balancing and traffic routing 
	Monitoring container health
	Configuring applications based on the container in which they will run
	Keeping interactions between containers secure
	batch execution (manifest are used like recipe)

k8s is  Container as a Service (CaaS) . 
CaaS is cloud service that allows software developers and IT departments to upload, organize, run, scale, manage and stop containers by using container-based virtualization

Alternatives:
	AWS Fargate.
	Azure Container Instances. 
	Google Cloud Run.
	Google Kubernetes Engine (GKE) 
	Amazon Elastic Kubernetes Service (EKS) 
	Openshift Container Platform. 
	Rancher. 
	Docker Swarm.
	Apache marathon 


Kubernetes comparison with its top compatitor docker swarm :

	it has gui but swarm does not have
	it is complex and harder than swarm but it has more features than swarm
	data sharabilty is only within pod in case of kubernetes but in swarm it is sharable throughout all containers
	swarm only supports Docker only while k8s supports  Docker, containerd, CRI-O, and any implementation of the Kubernetes CRI 
	it has support of auto scaling in both direction but swarm does not have
	k8s has its own monitering tool but swarm needs 3rd party tool to perform this


Architecture :

	it has master- node architecture inside a cluter , there can be more than 1 master in this architecture
	, containers communicates with pods , diff pods can have diff types of container services running in it , 
	like in 1 pod 20 docker containers are running but in 2 nd pod from same cluster can have 30 OpenVZ or 
			containerd conatiners running at same time 
	containers run microservice 


								  ------Cluster----------
								  |			 |
								master 		node --------------------------           (it can have multiple nodes for multiple purpoes)
											|                |            |
											pod1 	    	pod2    	pod3 		(generally all pods runs same application from same node)
											  | 		      |		    |		
											cont1		   cont1		cont1 		(diff types of container services can be used )
											cont2


	cluster -> nodes -> namesapce -> pods -> containers

	Kubernetes - Master Machine Components


	
			# Controller Manager
					This component is responsible for most of the collectors that regulates the state of cluster and performs a task.
					In general, it can be considered as a daemon which runs in non-terminating loop and 
							is responsible for collecting and sending information to API server. 

					It works toward getting the shared state of cluster and then make changes to bring the "current status" of
					  the server to "desired state".

					The controller manager runs different kind of controllers to handle nodes, endpoints, etc.
						like for every kind different controller is used

				


			# etcd
					It stores the configuration information which can be used by each of the nodes in the cluster. 
					It is a high availability key value store that can be distributed among multiple nodes. 
					It is accessible only by Kubernetes API server as it may have some sensitive information. 
					It is a distributed key value Store which is accessible to all.
					it is used to store cluster data so that in case if one of the node or even master fails it can be recovered.
					in EKS . every EKS cluster has its own etcd cluster , so it is not shared among nodes
					as etcd is open source , it is used by many other projects also like coreOs container , uber M3 app

					  - mariaDB can be used to backup resources of k8s like etcd
			
			# Scheduler
					This is one of the key components of Kubernetes master. 
					It is a service in master responsible for distributing the workload. 
					It is responsible fortracking utilization of working load on cluster nodes and 
						then placing the workload on which resources are available and accept the workload.
					The scheduler is responsible for workload utilization and allocating pod to new node.

	
			# API Server
					Kubernetes is an API server which provides all the operation on cluster using the API. 
					API server implements an interface, which means different tools and libraries can readily communicate with it. 
					Kubeconfig is a package along with the server side tools that can be used for communication. 
					It exposes Kubernetes API.
					it is designed to scale horizontally that is by adding more servers/Instances
					multiple API servers can be used to provide balcaned traffic between instances
					can be accessed via  UI , API , CLI (kubectl)

A minimum Kubernetes master node configuration is:
	4 CPU cores (Intel VT-capable CPU)
	16GB RAM.
	1GB Ethernet NIC.
	40GB hard disk space in the /var directory.

On AWS
	sizes we use on AWS are(master requirements)

		1-5 nodes: m3.medium
		6-10 nodes: m3.large
		11-100 nodes: m3.xlarge
		101-250 nodes: m3.2xlarge
		251-500 nodes: c4.4xlarge
		more than 500 nodes: c4.8xlarge

				Master									Node


					it has all data about nodes 			 		  	sends data
						ETCD (can be accessed only by apiserver)   <-------------   Kubelet(each for every node)
					         					 â†‘						 / --- Node 1  ---  pod11  -> cont1 , cont2
it checks etcd via server and commands           |						/
	 sheduler to perform actions		 		 |                     /				  
				CONTROL MANAGER	-------->   API SERVER ------------------------------ Node2  ----pod21  -> cont1 
						|							   |
						|							    -------- pod22  -> 	cont1 , cont2 , cont3			
						SCHEDULER (actions taken here actually)	

						communication from and to API server is done via HTTPs
						communication between pods is done using gRPC   (Google's Remote procedure Protocol) like	
								- kubelet to container engine/runtime like docker daemon
								- kubelet to container storage interface (CSI used to access Volumes in pods)
							

	

		* Admin sends menifest to Api server then 
	control manager check actual and desired state with help of etcd and 
	commands scheduler to perform necessory action to get in desired state
	scheduler smartly decides which pod to run on which node and which container to run on which pod based on the workloads on each cluster & node
	but we can also decide manually to on which node pod will be created 
	by getting commnds from scheduler , kubelet on nodes perform actions to create pod based on image provided by scheduler itself
	Replication is done by Control manager by checking the desired and actual state of node and if any of pod fails ,new pod get scheduled on node
	etcd contains status of every node which is deliverd by kubelet from each nodes

		in case of 2 master structuer , api-server is getting load balanced and etcd is being shared by both master
	

	We can add additional master or worker node anytime in architecture , we can just to by installing required specification


	Kubernetes - Node Components


			# Kubelet Service (sends node status to etcd) 10225 port by default

					This is a small service in each node responsible for relaying information to and from control plane service. 
					It interacts with etcd store to read configuration details and write values. 
					it communicates via API sevrer not diretly to etcd
					This communicates with the master component to receive commands and work. 
					The kubelet process then assumes responsibility for maintaining the state of work and the node server. 
					It manages network rules, port forwarding, etc.
					it also configure container runtime to pull and run image , create namespaces 
					it uses PodSpec file to determine what image to pull and container to run 
					it also sends back HTTPs request to API server about container logs and execution requests 


			# Kubernetes Proxy Service (each pod has ip to communicate with other pods or external network )

					This is a proxy service which runs on each node and helps in making services available to the external host. It helps in forwarding the request to 					
					correct containers and is capable of performing primitive load balancing. 
					It makes sure that the networking environment is predictable and accessible and at the same time it is isolated as well. 
					It manages pods on node, volumes, secrets, creating new containersâ€™ health checkup, etc.

		
			# Container Engine (e.g, Docker)

					The first requirement of each node is Docker which helps in running the encapsulated application containers in a relatively isolated but lightweight operating environment.




//////    Kubernetes services 


					////  ***  kubectl    ***   ///
		
		The Kubernetes command-line tool, kubectl, allows you to run commands against Kubernetes clusters. 
		You can use kubectl to deploy applications, inspect and manage cluster resources, and view logs
		kubectl communicated with api-server to perform actions based on commands 
		it is configured using $HOME/.kube/config    file





					////  ***  kubeadm    ***   ///
			You can use the kubeadm tool to create and manage Kubernetes clusters. 
			It performs the actions necessary to get a minimum viable, secure cluster up and running in a user friendly way.


					////  ***  Kops    ***   ///

			Kops, short for Kubernetes Operations, is a set of tools for installing, operating, and deleting Kubernetes clusters in the cloud.
			A rolling upgrade of an older version of Kubernetes to a new version can also be performed. It also manages the cluster add-ons.


					////  ***  kublet    ***   ///

		kubelet: the component that runs on all of the machines/nodes in your cluster and does things like starting pods and containers.
		it is responsible for keeping the state of the node to desired state
		Also if user has to access container programatically , then kubelet is responsible for that


					////  ***  kubProxy    ***   ///

		it ensure that the pods are communicating with rights containers from other pods in same cluster 

		it also makes communication between service and pods

		This is a proxy service which runs on each node and helps in making services available to the external host. 
		
		It helps in forwarding the request to correct containers and is capable of performing primitive load balancing. 
		It makes sure that the networking environment is predictable and accessible and at the same time it is isolated as well. 
		
		It manages pods on node, volumes, secrets, creating new containersâ€™ health checkup, etc.

		it is designed to load balance traffic to pods 

		it can run in 2 modes :
			1. iptable   : suitable for simple and less services 
			2. ipvs      :  suites 1000+ services , future replacement of iptable
	




HPA  (horizontal pod autoscale)

using resource metric (like cpu , memory) , we can do autoscaling of pods

in order to use HPA , we nedd to install metric server on cluster

here we set desired value and if the current value is more than desired value then it will scale up the pod

e.g. if current metric value is 200m and desired value is 100m then replicas will be doubled, since 200.0 / 100.0 == 2.0

in case of downscaling , we have cooldown period 
if the current value is less than desired value for cooldown period of time then it will scale down the pod





Tools can be used in Kubenetes :

	Logging : FluentD , FluentBit
	Monitering : DataDog , new Relic
	Cost : Kubecost , CloudHealth
	Storage : Portworx , Velero
	Security : Aqua , Twistlock
	Devops ; jenkins , gitlab


Kubernaetes On Cloud 

	AWS :
		ECS (Elastic Container Service)  => control plane costs zero , only pay for workernode ($70 per month per node)
											Requires creating cluster
 
		EKS(Elastic Kubernetes Service)	=> control plane costs $144/month , only pay for workernode , difficult UI but powerful CLI (EKSCTL)
											Requires creating cluster

		Farget     => no cluster required , 

	GCP :
		GKE (google k8s Engine)

	Azure :
			AKS (Azure kubernetes service)

	IBM  :
		IBM cloud k8s service  =>  not feature rich but more expensive than other cloud providers

	Oracle container Engine for k8s => cost efficient but limited options

	DOKS (digital ocean) : predictable spending , good UI 


Additional terms:

	In-Tree : plugins , components  or functions that are provided by default or resides within main repo
	Out-tree : plugins , components  or functions that must be installed manually

	gRPC is a high performance, open-source universal RPC(remote procedure call) framework, based on the TCP communication developed by google 
	mainly used for communicate betn distributed system
	used by installing gRPC library 
	k8s uses for pod communication 

	management layer : allows to extend control plane to multiple platform 
			e.g.  Weave K8s platform , Rafay  , Vmware Tanzu , RadHat Openshift (Paas for k8s)

	Container Runtime :  (interface + runtime)
		kubelet wants to process pod specs, it needs a container runtime to create the actual containers. 
		The runtime is then responsible for managing the container lifecycle and communicating with the operating system kernel.

	CRI (Conatiner runtime interface) : allows to run diff. container runtimes like Docker, CRI-O , containerd
	CRI enables Kubernetes users to easily make use of multiple container runtimes, and 
			enables developers of container runtimes to easily integrate them with the Kubernetes ecosystem. 
	Prior to the introduction of the CRI, rkt and Docker were directly integrated into the source code of the kubelet

		ContainerD : industry standerd container runtime with robust and portable features donated to CNCF by Docker

	Open Container Initiative (OCI) is a Linux Foundation project to design open standards for containers.
	it is industry standerd container image format and runtime to make sure that all conatainer runtime could run image produced by any build tool

	Open Container initiative (OCI) Runtime : 
			it is low level interface for container runtime , e.g. runC is used by ContainerD and CRI-O
			it has 2 types of runtime :
				Native : e.g. runC , one kernal for all containers
				Virtualized :  e.g. kata   , one per container linux kernal

		RunC is low level container runtime that creates and run containers  ,  used alogside a ContainerD or CRI-O

	
	Container storage interface (CSI) : standerdize how orchestration toll like k8s access to various storage providers like AWS EBS , AZure disk

	Service Mesh : service - to - service communication for microservice architecture  (e.g. NATS , gRPC , REST , SOAP)
					on each pod proxy container is installed , an application must pass through proxy before leaving engress
					proxy installation on pod is done by service mesh control plane

				e.g. Istio , uses Envoy , not CNCF project

	Container Network Interface (CNI) : CNI plugin is responsible for inserting a network interface into the container network namespace 
							(e.g., one end of a virtual ethernet (veth) pair) 
								and making any necessary changes on the host (e.g., attaching the other end of the veth into a bridge). 
						It then assigns an IP address to the interface and sets up the routes consistent with the IP Address Management section 
								by invoking the appropriate IP Address Management (IPAM) plugin.

			container/pod initially has no network interface. The container runtime calls the CNI plugin with verbs such as ADD, DEL, CHECK, etc. 
			ADD creates a new network interface for the container, and details of what is to be added are passed to CNI via JSON payload.

	 network namespaces : logical isolation of network , heavily used in pod to pod communication , every pod has its own namespace 
	 								virtual Ethernet is used to peer/bridge connection between pod and node and further to another node or pod
									 it allows using same private ip in distinct network namespaces



							////  ***  K3s   ***   ///  

			it is lightweight k8s distribution designed to run production level k8s workload for low resourced and remotely located Iot Devices and edge devices  
			it uses hosts scheduling mechanism instead of kubelet to run containers , it is bare metal
			k3d(minikube Alternative) run docker container  , requires docker installed




					////  ***  microK8s   ***   ///  

	        (can be used in self-hosted production use case)
			k8s distribution created by canonical (company behind ubuntu OS) and installed using snap
			restarts if crashes

			trick : use              microk8s > ~/.kube/config 

										to use kubectl dirctly instead of microk8s kubectl




					////  ***  kind    ***   ///  (kubernetes in docker)  only for development
		
		kind lets you run Kubernetes on your local computer. This tool requires that you have Docker installed and configured.
		uses docker containers as node

			


					////  ***   minikube   ***   ///    only for development

	Like kind, minikube is a tool that lets you run Kubernetes locally as a virtual machine
	minikube runs a single-node Kubernetes cluster on your personal computer (including Windows, macOS and Linux PCs) 
			so that you can try out Kubernetes, or for daily development work.
	it has docker container time preinstalled so we dont need to install it but it is only available inside minikube 
		there will be different docker registry of docker inside minikube and locally installed docker

	use --vm-driver along with start to select hypervisor where minikube requires  , it is deprecated as  --driver
	--driver='': Driver is one of: virtualbox, vmwarefusion, kvm2, vmware,none, docker, podman, ssh (defaults to auto-detect)

	using : eval $(minikube docker-env)     , we can access docker inside minikube for that opened cli only 

				set imagePullPolicy to Never ,otherwise Kubernetes will try to download the image into its docker 

				to pull from local docker registry use :   minikube image load IMG_NAME            

				it will load the image into minikube docker registry from our local docker registry



		C-Group (controls groups) : allows to group processes to apply different kinds of limitations like 
											resource limiting , prioritize cpu utilization
									Goal is to provide unified interface to manage whole os level virtualization 

		CoreDNS is a flexible, extensible DNS server that can serve as the Kubernetes cluster DNS. 
		Like Kubernetes, the CoreDNS project is hosted by the CNCF.
		You can use CoreDNS instead of kube-dns in your cluster by replacing kube-dns in an existing deployment, 
				or by using tools like kubeadm that will deploy and upgrade the cluster for you
		kube-dns is defaultly used with k8s , to use CORE-DNS we need to first install it manually
		CoreDNS has plugins to support many of functionalities with other services

		Rook :  Open-Source,  Cloud-Native Storage for Kubernetes 
				Rook turns distributed storage systems into self-managing, self-scaling, self-healing storage services. 
				It automates the tasks of a storage administrator: 
						deployment, bootstrapping, configuration, provisioning, scaling, upgrading, migration, disaster recovery, monitoring, and resource management.
				Rook uses the power of the Kubernetes platform to deliver its services via a Kubernetes Operator for each storage provider.

		MinIO 
				it offers high-performance, S3 compatible object storage.
				Native to Kubernetes, MinIO is the only object storage suite available on
				every public cloud, every Kubernetes distribution, the private cloud and the edge


Knative :  Knative is an Open-Source Enterprise-level solution to build Serverless and Event Driven Applications
           also known as Serverless Containers in Kubernetes environments.
		   Knative allows you to create serverless environments using containers , same as OpenFaaS
		   The Knative architecture consists of the Building, Eventing, and Serving components.


OpenFaas :   OpenFaaS brings Functions As A Service (FaaS) to Kubernetes
			OpenFaas ( Function as a Service) is a framework for building serverless functions on the top of containers(with docker and kubernetes). 
			With the help of OpenFaas, 
				it is easy to turn anything into a serverless function that runs on Linux or windows through Docker or Kubernetes.
			OpenFaaSÂ® makes it easy for developers to deploy event-driven functions and microservices 
					to Kubernetes without repetitive, boiler-plate coding.
		OpenFaaS architecture is based on a cloud-native standard and includes the following components: 
				API Gateway 
				Function Watchdog
				container orchestrators like Kubernetes, Docker Swarm
				Prometheus
				Docker 
		
		process begins with the installation of Docker and ends with the Gateway API.


 


inaccel

keda
kubeflow
metallb
openebs
prometheus


Cilium: API-aware networking and security for containers. 
		Cilium is an open source software for providing, securing and observing network connectivity between container workloads 
				- cloud native, and fueled by the revolutionary Kernel technology eBPF
		Open source software for providing and transparently securing network connectivity 
				and loadbalancing between application workloads such as application containers or processes 

eBPF is a revolutionary technology with origins in the Linux kernel that can run sandboxed programs in an operating system kernel. 
It is used to safely and efficiently extend the capabilities of the kernel without requiring to change kernel source code or load kernel modules.

Istio: Open platform to connect, manage, and secure microservices, by Google, IBM, and Lyft. 
		Istio is an open platform for providing a uniform way to integrate microservices, 
			manage traffic flow across microservices, enforce policies and aggregate telemetry data. 
		Istio's control plane provides an abstraction layer over the underlying cluster management platform, such as Kubernetes, Mesos, etc.

Cilium can be classified as a tool in the "Security" category, while Istio is grouped under "Microservices Tools".


Antrea is a Kubernetes-native project that implements the Container Network Interface (CNI) and Kubernetes NetworkPolicy 
	thereby providing network connectivity and security for pod workloads. 
Antrea extends the benefit of programmable networks from Open vSwitch (OVS) to Kubernetes.
it is used for Enable pod networking and enforce network policies for Kubernetes clusters


Calico is an open source networking and network security solution for containers, virtual machines, and native host-based workloads. 
Calico supports a broad range of platforms including Kubernetes, OpenShift, Mirantis Kubernetes Engine (MKE), OpenStack, and bare metal services.
Calico gives you a choice of dataplanes, including 
		1. pure Linux eBPF dataplane       2.  a standard Linux networking dataplane   3.  Windows HNS dataplane. 
Whether you prefer cutting edge features of eBPF, or the familiarity of the standard primitives 
	that existing system administrators already know, Calico has you covered.
Whichever choice is right for you, youâ€™ll get the same, easy to use, base networking, network policy and IP address management capabilities, 
	that have made Calico the most trusted networking and network policy solution for mission-critical cloud-native applications.


Linker-D :
		fast and light service mesh for Kubernetes , alternatiev to istio
