******************************************************************************************************************************************************************************************
										About Kubernetes
******************************************************************************************************************************************************************************************


* imperative means use cli every time to execute , declarative means make file and then execute it every time 

Cloud Native : architectural approach conatains workload that is poratable , modular and isolate between differnent cloud providers and cloud deployment models

k8s is based on cloud native

The Linux Foundation :
			formed in 2000's by merger between Open source development labs and Free student groups to support linux's growth and commerercial adoption 

CNCF (Cloud native Computing Foundation):
			formed in 2015 as a linux Foundation project to help advance container technology ,it operates independently from parent orgaization 


K8s , Prometheus , ETCD, ConaitnerD ,HELM  are some of the projects by CNCF

simply container management/orchestration tool , also known as k8s , Container orchestration automates the deployment, management, scaling, and networking of containers.
orchestration means clustering of any no of containers running on any network , it can works on also hybrid cloud also
Kubernetes was originally developed and designed by engineers at Google. it was developed in Golang , it supports menifest scripts in JSON and Yaml

container orchestration to automate and manage tasks such as:

	Provisioning and deployment
	Configuration and scheduling 
	Resource allocation
	Container availability 
	Scaling or removing containers based on balancing workloads across your infrastructure (can scale vertically[amount of resources] as well horizontally[no of containers])
	Load balancing and traffic routing 
	Monitoring container health
	Configuring applications based on the container in which they will run
	Keeping interactions between containers secure
	batch execution (manifest are used like recipe)

k8s is  Container as a Service (CaaS) . 
CaaS is cloud service that allows software developers and IT departments to upload, organize, run, scale, manage and stop containers by using container-based virtualization

Alternatives:
	AWS Fargate.
	Azure Container Instances. 
	Google Cloud Run.
	Google Kubernetes Engine (GKE) 
	Amazon Elastic Kubernetes Service (EKS) 
	Openshift Container Platform. 
	Rancher. 
	Docker Swarm.
	Apache marathon 


Kubernetes comparison with its top compatitor docker swarm :

	it has gui but swarm does not have
	it is complex and harder than swarm but it has more features than swarm
	data sharabilty is only within pod in case of kubernetes but in swarm it is sharable throughout all containers
	swarm only supports Docker only while k8s supports  Docker, containerd, CRI-O, and any implementation of the Kubernetes CRI 
	it has support of auto scaling in both direction but swarm does not have
	k8s has its own monitering tool but swarm needs 3rd party tool to perform this


Architecture :

	it has master- node architecture inside a cluter , there can be more than 1 master in this architecture
	, containers communicates with pods , diff pods can have diff types of container services running in it , 
	like in 1 pod 20 docker containers are running but in 2 nd pod from same cluster can have 30 OpenVZ or containerd conatiners running at same time 
	containers run microservice 


								  ------Cluster----------
								  |			 |
								master 		node --------------------------           (it can have multiple nodes for multiple purpoes)
											|                |            |
											pod1 	    	pod2    	pod3 		(generally all pods runs same application from same node)
											  | 		      |		    |		
											cont1		   cont1		cont1 		(diff types of container services can be used )
											cont2


	cluster -> nodes -> namesapce -> pods -> containers

	Kubernetes - Master Machine Components


	
			# Controller Manager
					This component is responsible for most of the collectors that regulates the state of cluster and performs a task.
					In general, it can be considered as a daemon which runs in non-terminating loop and 
							is responsible for collecting and sending information to API server. 

					It works toward getting the shared state of cluster and then make changes to bring the "current status" of
					  the server to "desired state".

					The controller manager runs different kind of controllers to handle nodes, endpoints, etc.
						like for every kind different controller is used

				


			# etcd
					It stores the configuration information which can be used by each of the nodes in the cluster. 
					It is a high availability key value store that can be distributed among multiple nodes. 
					It is accessible only by Kubernetes API server as it may have some sensitive information. 
					It is a distributed key value Store which is accessible to all.
					it is used to store cluster data so that in case if one of the node or even master fails it can be recovered.
					in EKS . every EKS cluster has its own etcd cluster , so it is not shared among nodes
					as etcd is open source , it is used by many other projects also like coreOs container , uber M3 app

					  - mariaDB can be used to backup resources of k8s like etcd
			
			# Scheduler
					This is one of the key components of Kubernetes master. 
					It is a service in master responsible for distributing the workload. 
					It is responsible fortracking utilization of working load on cluster nodes and 
						then placing the workload on which resources are available and accept the workload.
					The scheduler is responsible for workload utilization and allocating pod to new node.

	
			# API Server
					Kubernetes is an API server which provides all the operation on cluster using the API. 
					API server implements an interface, which means different tools and libraries can readily communicate with it. 
					Kubeconfig is a package along with the server side tools that can be used for communication. 
					It exposes Kubernetes API.
					it is designed to scale horizontally that is by adding more servers/Instances
					multiple API servers can be used to provide balcaned traffic between instances
					can be accessed via  UI , API , CLI (kubectl)

A minimum Kubernetes master node configuration is:
	4 CPU cores (Intel VT-capable CPU)
	16GB RAM.
	1GB Ethernet NIC.
	40GB hard disk space in the /var directory.

On AWS
	sizes we use on AWS are(master requirements)

		1-5 nodes: m3.medium
		6-10 nodes: m3.large
		11-100 nodes: m3.xlarge
		101-250 nodes: m3.2xlarge
		251-500 nodes: c4.4xlarge
		more than 500 nodes: c4.8xlarge

				Master									Node


					it has all data about nodes 			 		  	sends data
						ETCD (can be accessed only by apiserver)   <-------------   Kubelet(each for every node)
					         					 ↑						 / --- Node 1  ---  pod11  -> cont1 , cont2
it checks etcd via server and commands           |						/
	 sheduler to perform actions		 		 |                     /				  
				CONTROL MANAGER	-------->   API SERVER ------------------------------ Node2  ----pod21  -> cont1 
						|							   |
						|							    -------- pod22  -> 	cont1 , cont2 , cont3			
						SCHEDULER (actions taken here actually)	

						communication from and to API server is done via HTTPs
						communication between pods is done using gRPC   (Google's Remote procedure Protocol) like	
								- kubelet to container engine/runtime like docker daemon
								- kubelet to container storage interface (CSI used to access Volumes in pods)
							

	

		* Admin sends menifest to Api server then 
		control manager check actual and desired state with help of etcd and 
		commands scheduler to perform necessory action to get in desired state
		scheduler smartly decides which pod to run on which node and which container to run on which pod based on the workloads on each cluster and node
		but we can also decide manually to on which node pod will be created 
		by getting commnds from scheduler , kubelet on nodes perform actions to create pod based on image provided by scheduler itself
		Replication is done by Control manager by checking the desired and actual state of node and if any of pod fails ,new pod get scheduled on node
		etcd contains status of every node which is deliverd by kubelet from each nodes

		in case of 2 master structuer , api-server is getting load balanced and etcd is being shared by both master
	

	We can add additional master or worker node anytime in architecture , we can just to by installing required specification


	Kubernetes - Node Components


			# Kubelet Service (sends node status to etcd) 10225 port by default

					This is a small service in each node responsible for relaying information to and from control plane service. 
					It interacts with etcd store to read configuration details and write values. 
					it communicates via API sevrer not diretly to etcd
					This communicates with the master component to receive commands and work. 
					The kubelet process then assumes responsibility for maintaining the state of work and the node server. 
					It manages network rules, port forwarding, etc.
					it also configure container runtime to pull and run image , create namespaces 
					it uses PodSpec file to determine what image to pull and container to run 
					it also sends back HTTPs request to API server about container logs and execution requests 


			# Kubernetes Proxy Service (each pod has ip to communicate with other pods or external network )

					This is a proxy service which runs on each node and helps in making services available to the external host. It helps in forwarding the request to 					
					correct containers and is capable of performing primitive load balancing. 
					It makes sure that the networking environment is predictable and accessible and at the same time it is isolated as well. 
					It manages pods on node, volumes, secrets, creating new containers’ health checkup, etc.

		
			# Container Engine (e.g, Docker)

					The first requirement of each node is Docker which helps in running the encapsulated application containers in a relatively isolated but lightweight operating environment.




//////    Kubernetes services 


					////  ***  kubectl    ***   ///
		
		The Kubernetes command-line tool, kubectl, allows you to run commands against Kubernetes clusters. 
		You can use kubectl to deploy applications, inspect and manage cluster resources, and view logs
		kubectl communicated with api-server to perform actions based on commands 
		it is configured using $HOME/.kube/config    file





					////  ***  kubeadm    ***   ///
			You can use the kubeadm tool to create and manage Kubernetes clusters. 
			It performs the actions necessary to get a minimum viable, secure cluster up and running in a user friendly way.


					////  ***  Kops    ***   ///

			Kops, short for Kubernetes Operations, is a set of tools for installing, operating, and deleting Kubernetes clusters in the cloud.
			A rolling upgrade of an older version of Kubernetes to a new version can also be performed. It also manages the cluster add-ons.


					////  ***  kublet    ***   ///

		kubelet: the component that runs on all of the machines/nodes in your cluster and does things like starting pods and containers.
		it is responsible for keeping the state of the node to desired state
		Also if user has to access container programatically , then kubelet is responsible for that


					////  ***  kubProxy    ***   ///

		it ensure that the pods are communicating with rights containers from other pods in same cluster 

		it also makes communication between service and pods

		This is a proxy service which runs on each node and helps in making services available to the external host. 
		
		It helps in forwarding the request to correct containers and is capable of performing primitive load balancing. 
		It makes sure that the networking environment is predictable and accessible and at the same time it is isolated as well. 
		
		It manages pods on node, volumes, secrets, creating new containers’ health checkup, etc.

		it is designed to load balance traffic to pods 

		it can run in 2 modes :
			1. iptable   : suitable for simple and less services 
			2. ipvs      :  suites 1000+ services , future replacement of iptable
	




HPA  (horizontal pod autoscale)

using resource metric (like cpu , memory) , we can do autoscaling of pods

in order to use HPA , we nedd to install metric server on cluster

here we set desired value and if the current value is more than desired value then it will scale up the pod

e.g. if current metric value is 200m and desired value is 100m then replicas will be doubled, since 200.0 / 100.0 == 2.0

in case of downscaling , we have cooldown period 
if the current value is less than desired value for cooldown period of time then it will scale down the pod





Tools can be used in Kubenetes :

	Logging : FluentD , FluentBit
	Monitering : DataDog , new Relic
	Cost : Kubecost , CloudHealth
	Storage : Portworx , Velero
	Security : Aqua , Twistlock
	Devops ; jenkins , gitlab


Kubernaetes On Cloud 

	AWS :
		ECS (Elastic Container Service)  => control plane costs zero , only pay for workernode ($70 per month per node)
											Requires creating cluster
 
		EKS(Elastic Kubernetes Service)	=> control plane costs $144/month , only pay for workernode , difficult UI but powerful CLI (EKSCTL)
											Requires creating cluster

		Farget     => no cluster required , 

	GCP :
		GKE (google k8s Engine)

	Azure :
			AKS (Azure kubernetes service)

	IBM  :
		IBM cloud k8s service  =>  not feature rich but more expensive than other cloud providers

	Oracle container Engine for k8s => cost efficient but limited options

	DOKS (digital ocean) : predictable spending , good UI 


Additional terms:

	In-Tree : plugins , components  or functions that are provided by default or resides within main repo
	Out-tree : plugins , components  or functions that must be installed manually

	gRPC is a high performance, open-source universal RPC(remote procedure call) framework, based on the TCP communication developed by google 
	mainly used for communicate betn distributed system
	used by installing gRPC library 
	k8s uses for pod communication 

	management layer : allows to extend control plane to multiple platform 
			e.g.  Weave K8s platform , Rafay  , Vmware Tanzu , RadHat Openshift (Paas for k8s)

	Container Runtime :  (interface + runtime)
		kubelet wants to process pod specs, it needs a container runtime to create the actual containers. 
		The runtime is then responsible for managing the container lifecycle and communicating with the operating system kernel.

	CRI (Conatiner runtime interface) : allows to run diff. container runtimes like Docker, CRI-O , containerd
	CRI enables Kubernetes users to easily make use of multiple container runtimes, and enables developers of container runtimes to easily integrate them with the Kubernetes ecosystem. 
	Prior to the introduction of the CRI, rkt and Docker were directly integrated into the source code of the kubelet

		ContainerD : industry standerd container runtime with robust and portable features donated to CNCF by Docker

	Open Container Initiative (OCI) is a Linux Foundation project to design open standards for containers.
	it is industry standerd container image format and runtime to make sure that all conatainer runtime could run image produced by any build tool

	Open Container initiative (OCI) Runtime : 
			it is low level interface for container runtime , e.g. runC is used by ContainerD and CRI-O
			it has 2 types of runtime :
				Native : e.g. runC , one kernal for all containers
				Virtualized :  e.g. kata   , one per container linux kernal

		RunC is low level container runtime that creates and run containers  ,  used alogside a ContainerD or CRI-O

	
	Container storage interface (CSI) : standerdize how orchestration toll like k8s access to various storage providers like AWS EBS , AZure disk

	Service Mesh : service - to - service communication for microservice architecture  (e.g. NATS , gRPC , REST , SOAP)
					on each pod proxy container is installed , an application must pass through proxy before leaving engress
					proxy installation on pod is done by service mesh control plane

				e.g. Istio , uses Envoy , not CNCF project

	Container Network Interface (CNI) : CNI plugin is responsible for inserting a network interface into the container network namespace 
							(e.g., one end of a virtual ethernet (veth) pair) and making any necessary changes on the host (e.g., attaching the other end of the veth into a bridge). 
							It then assigns an IP address to the interface and sets up the routes consistent with the IP Address Management section 
									by invoking the appropriate IP Address Management (IPAM) plugin.

					 container/pod initially has no network interface. The container runtime calls the CNI plugin with verbs such as ADD, DEL, CHECK, etc. 
					 ADD creates a new network interface for the container, and details of what is to be added are passed to CNI via JSON payload.

	 network namespaces : logical isolation of network , heavily used in pod to pod communication , every pod has its own namespace 
	 								virtual Ethernet is used to peer/bridge connection between pod and node and further to another node or pod
									 it allows using same private ip in distinct network namespaces



							////  ***  K3s   ***   ///  

			it is lightweight k8s distribution designed to run production level k8s workload for low resourced and remotely located Iot Devices and edge devices  
			it uses hosts scheduling mechanism instead of kubelet to run containers , it is bare metal
			k3d(minikube Alternative) run docker container  , requires docker installed




					////  ***  microK8s   ***   ///  

	        (can be used in self-hosted production use case)
			k8s distribution created by canonical (company behind ubuntu OS) and installed using snap
			restarts if crashes




					////  ***  kind    ***   ///  (kubernetes in docker)  only for development
		
		kind lets you run Kubernetes on your local computer. This tool requires that you have Docker installed and configured.
		uses docker containers as node

			


					////  ***   minikube   ***   ///    only for development

		Like kind, minikube is a tool that lets you run Kubernetes locally as a virtual machine
		minikube runs a single-node Kubernetes cluster on your personal computer (including Windows, macOS and Linux PCs) so that you can try out Kubernetes, 
		or for daily development work.
		it has docker container time preinstalled so we dont need to install it but it is only available inside minikube 
		there will be different docker registry of docker inside minikube and locally installed docker

		use --vm-driver along with start to select hypervisor where minikube requires  , it is deprecated as  --driver
		--driver='': Driver is one of: virtualbox, vmwarefusion, kvm2, vmware,none, docker, podman, ssh (defaults to auto-detect)

		using : eval $(minikube docker-env)     , we can access docker inside minikube for that opened cli only 

				set imagePullPolicy to Never ,otherwise Kubernetes will try to download the image into its docker 

				to pull from local docker registry use :   minikube image load IMG_NAME            

				it will load the image into minikube docker registry from our local docker registry



		C-Group (controls groups) : allows to group processes to apply different kinds of limitations like resource limiting , prioritize cpu utilization
									Goal is to provide unified interface to manage whole os level virtualization 

		CoreDNS is a flexible, extensible DNS server that can serve as the Kubernetes cluster DNS. Like Kubernetes, the CoreDNS project is hosted by the CNCF.
		You can use CoreDNS instead of kube-dns in your cluster by replacing kube-dns in an existing deployment, or by using tools like kubeadm that will deploy and upgrade the cluster for you
		kube-dns is defaultly used with k8s , to use CORE-DNS we need to first install it manually
		CoreDNS has plugins to support many of functionalities with other services

		Rook :  Open-Source,  Cloud-Native Storage for Kubernetes 
				Rook turns distributed storage systems into self-managing, self-scaling, self-healing storage services. 
				It automates the tasks of a storage administrator: 
						deployment, bootstrapping, configuration, provisioning, scaling, upgrading, migration, disaster recovery, monitoring, and resource management.
				Rook uses the power of the Kubernetes platform to deliver its services via a Kubernetes Operator for each storage provider.

		MinIO 
				it offers high-performance, S3 compatible object storage.
				Native to Kubernetes, MinIO is the only object storage suite available on
				every public cloud, every Kubernetes distribution, the private cloud and the edge